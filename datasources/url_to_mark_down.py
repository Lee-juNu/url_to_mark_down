import requests
from bs4 import BeautifulSoup
from collections.abc import Generator
from typing import Any, Mapping

# Dify OSS SDK í˜¸í™˜ìš© shim
try:
    from dify_plugin.interfaces.datasource.website import WebsiteCrawlDatasource
except ImportError:
    from dify_plugin import Datasource as WebsiteCrawlDatasource

from dify_plugin.entities.datasource import (
    WebSiteInfo,
    WebSiteInfoDetail,
    WebsiteCrawlMessage,
)

import html2text
import re


class URLToMarkdownDatasource(WebsiteCrawlDatasource):
    """Fetches the given URL and extracts readable Markdown content optimized for RAG ingestion."""

    def _get_website_crawl(
        self, datasource_parameters: Mapping[str, Any]
    ) -> Generator[WebsiteCrawlMessage, None, None]:
        source_url = datasource_parameters.get("url")
        if not source_url:
            raise ValueError("url is required")

        crawl_res = WebSiteInfo(web_info_list=[], status="processing", total=1, completed=0)
        yield self.create_crawl_message(crawl_res)

        try:
            resp = requests.get(source_url, timeout=15)
            resp.raise_for_status()

            soup = BeautifulSoup(resp.text, "html.parser")

            # ğŸ§¹ 1. ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ë…¸ì´ì¦ˆ ì œê±°
            for tag in soup(["script", "style", "noscript", "iframe", "header", "footer", "nav", "form"]):
                tag.decompose()

            # ğŸ§± 2. í—¤ë”ë¥¼ Markdown í—¤ë”ë¡œ ë³€í™˜ (chunk êµ¬ì¡° ë³´ì¡´)
            for header_tag in soup.find_all(["h1", "h2", "h3", "h4", "h5"]):
                prefix = "#" * int(header_tag.name[1])
                header_text = header_tag.get_text(strip=True)
                header_tag.insert_before(f"\n{prefix} {header_text}\n")

            # ğŸ§© 3. ë³¸ë¬¸ ì¶”ì¶œ í›„ html2text ë¡œ Markdown ë³€í™˜
            converter = html2text.HTML2Text()
            converter.ignore_links = False
            converter.ignore_images = False
            converter.body_width = 0
            markdown_text = converter.handle(str(soup))

            # ğŸª¶ 4. ë¶ˆí•„ìš”í•œ ë¹ˆ ì¤„, ê³µë°± ì •ë¦¬
            markdown_text = re.sub(r"\n{3,}", "\n\n", markdown_text)
            markdown_text = markdown_text.strip()

            # âœ‚ï¸ 5. ì²­í‚¹ ë‹¨ìœ„ ê°œì„ : í—¤ë” ë‹¨ìœ„ë¡œ ë¶„ë¦¬ í‘œì‹œ (RAG-friendly)
            #    ì˜ˆ: "# ì œëª©1\në‚´ìš©\n## ì†Œì œëª©\në‚´ìš©" â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ë˜ë„ë¡ í•¨
            #    -> í›„ë‹¨ ì„ë² ë”©ì—ì„œ TokenTextSplitterê°€ ì˜ ì¸ì‹

            # ğŸ§  6. ë©”íƒ€ì •ë³´ êµ¬ì„±
            title = soup.title.string.strip() if soup.title and soup.title.string else "Untitled"
            excerpt = markdown_text[:400].replace("\n", " ")

            info = WebSiteInfoDetail(
                source_url=source_url,
                content=markdown_text,
                title=title,
                description=excerpt,
            )

            crawl_res.web_info_list = [info]
            crawl_res.status = "completed"
            crawl_res.completed = 1
            yield self.create_crawl_message(crawl_res)

        except Exception as e:
            crawl_res.status = "failed"
            yield self.create_crawl_message(crawl_res)
            raise ValueError(f"Failed to fetch URL: {str(e)}")
